\section{Evaluation}
We evaluate the performance of \dltpy{} by creating an implementation, collecting training data, and training the model based on this data. We judge the results using the metrics precision, recall, and F1-score. We also perform experiments using variants of \dltpy{} for comparison.

\subsection{Implementation} \label{evaluation:implementation}
We implement \dltpy{} in Python. We use GitPython \cite{2008GitPython} for cloning libraries, astor \cite{2012Astor} to parse Python code to ASTs, docstring\_parser \cite{2018Docstring_parser} for extracting comment elements, and NLTK \cite{Bird2009NaturalPython} for lemmatization and removing stopwords. We train two word embedding models using the Word2Vec \cite{Mikolov2013EfficientSpace} library by gensim \cite{Rehurek2010SoftwareCorpora}. The data is represented using Pandas dataframes \cite{McKinney2010DataPython} and NumPy vectors \cite{vanderWalt2011TheComputation}. Finally, the prediction models are developed using PyTorch \cite{Paszke2017AutomaticPyTorch}, a machine learning framework.

We make the source for training and evaluating \dltpy{} publicly available at \url{https://github.com/casperboone/nl2pythontype/}.

\subsection{Experimental Setup}
We collect training data from open-source GitHub projects. We first select projects by looking at \texttt{mypy} \cite{2012Mypy} dependents in two ways.
First, we look at the dependents listed on GitHub's dependency graph \footnote{https://github.com/python/mypy/network/dependents}.
Then, we complete this list with the dependents listed by \texttt{Libraries.io} \footnote{https://libraries.io/pypi/mypy/usage}.
The intuition is that Python projects using the type checker \texttt{mypy} are more likely to have types than projects that do not have this dependency. This results in a list of 5,996 projects, of which we can download and process (section \ref{method:extract}) 5,922 projects. 74 projects fail, for instance due to the unavailability of a project on GitHub.

Together, these projects have 555,772 Python files. 5,054 files cannot be parsed.
The files contain 4,977,420 functions, of which only have 585,413 functions have types (at least one parameter type or a return type).

We complement this dataset with projects for which type hints are provided in the Typeshed repository\footnote{https://github.com/python/typeshed}. These are curated type annotations for the Python standard library and for certain Python packages. We manually find and link the source code of 35 packages, including the standard library. Using retype we insert the type annotations into the source code \cite{2017Retype}. These projects have 29,759 functions, of which 246 have types.

We randomly split this set of datapoints into a non-overlapping training set (80\%) and a test set (20\%). We repeat every experiment 3 times.

All experiments are conducted on a Windows 10 machine with an AMD Ryzen 9 3900X processor with 12 cores, 32 GB of memory, and an NVIDIA GeForce GTX 1080 Ti GPU with 11 GB of memory.

\subsection{Metrics} \label{evaluation:metrics}
We evaluate the performance based on the accuracy of predictions.
We measure the accuracy in terms of precision (the fraction of found results that are relevant), recall (the fraction of relevant results found) and F1-score (harmonic mean of precision and recall).
Because in many contexts (for example IDE auto-completion) it is not necessary to restrict to giving a single good prediction, we look at the top-$K$ predictions. 
Specifically, we collect the three metrics for the top-1, top-2, and top-3 predictions.

We define $p$ as the total number of predictions, $p_{valid}$ as the number of predictions for which the prediction is not \texttt{other} (and thus the model cannot make a prediction). We define the three metrics as follows: 

\begin{itemize}
    \item $top\text{-}K\text{ }precision =     
        \dfrac{
            p_{valid\_correct}
        }{
            p_{valid}
        }
        $
        , \\ where $p_{valid\_correct}$ is the number of valid predictions for which the correct prediction is in the top-$K$
    
    \item $top\text{-}K\text{ }recall =     
        \dfrac{
            p_{valid\_correct}
        }{
            p
        }
        $
        ,
    
    \item $top\text{-}K\text{ }F1 = 
        2 \times
        \dfrac{
            top\text{-}K\text{ }precision \times top\text{-}K\text{ }recall
        }{
             top\text{-}K\text{ }precision + top\text{-}K\text{ }recall
        }
        $
        .
\end{itemize}


\subsection{Experiments and Models} \label{evaluation:experiments}
While we mainly evaluate the performance of \dltpy{} as described in the previous section, we also try, evaluate and compare the results when we use different models or different input data. We train three different models and compare these to the model presented in section \ref{method:lstm}. Also, we evaluate the results of selecting different input elements.

\input{tables/datasets.tex}

\subsubsection{Models} \label{evaluation:experiments:models}
We implement and train three models to evaluate and compare their performance.
\begin{notsw}
\begin{itemize}
    % \item \textbf{Model A} The first model contains a bidirectional LSTM \cite{Hochreiter1997LongMemory} layer with a hidden size of 20. The output of the last unit is fed into a fully connected linear layer and softmax is applied to generate an approximation of the probability for each type. The model has a total of 46,760 parameters.
    
    \item \textbf{Model A} In the first model, we made use of two stacked LSTMs. Both LSTMs have a hidden size of 14. The first LSTM will feed its sequence fully into the second, whereas for the second LSTM we only use the output of the last unit and feed it into a fully connected linear layer. Softmax is applied to generate an approximation of the probability for each type. The model has a total of 37,288 parameters.
    
    \item \textbf{Model B} In the second model, we take an approach similar to Model A: we feed the input vector into a Gated Recurrent Unit (GRU) \cite{Cho2014LearningTranslation} and feed the output to a fully connected linear layer converting into the output types. The model has a total of 11,780 parameters.
    
    \item \textbf{Model C} The third model is the model architecture proposed by the authors of \cite{Malik2019NL2Type:Information}. A single bi-directional LSTM with a hidden layer of size 256 is fed into a fully connected layer with output size 1000. Due to the size of this model and the time it took to train, the training consisted of only 25 epochs, compared to 100 epochs for the other three models. The model has a total of 404,456 parameters.
\end{itemize}
\end{notsw}

\subsubsection{Datasets}

We try and evaluate variations in input data to measure the impact of certain elements in the datapoints. To this purpose, we create five datasets. The size of these datasets is listed in Table \ref{table:datasets}.


\begin{enumerate}
    \item \textbf{Complete} This is the dataset as described in section \ref{method}. All datapoints in this dataset are complete. This means that the parameter datapoints have $c_p$, and the return datapoints have $c_f$, $c_r$, and $e_r$. 
    
    \item \textbf{Optional parameter and return comment} In this dataset we make the parameter and return comment optional. The presence of a docstring is still required. This means that parameter datapoints do not have to have $c_p$ (91,01\%), and the return datapoints do not have to have $c_r$ (84,79\%), but still have $c_f$ (which is either the parsed function comment or the docstring) and $e_r$.
    
    \item \textbf{Optional docstring} In this dataset we make the docstring optional. This means that parameter datapoints do not have to have $c_p$ (91,01\%), and the return datapoints do not have to have $c_f$ and $c_r$ (92,66\%), but still have $e_r$ (51,75\%). This can be seen as a dataset without comments since only 20,52\% of the datapoints in this set have comments. This means that the prediction for parameters is purely based on the parameter name, and for return datapoints, the prediction is based on the function name, the return expressions, and the parameter names.
    
    \item \textbf{Without return expressions} To evaluate the usefulness of including return expressions in the model input, we perform the classification task also without return expressions. In this dataset all vectors representing parts of return expressions are 0-vectors.
    
    \item \textbf{Without return expressions, lower dimension} This dataset is similar to the previous one. In this dataset, however, all vectors representing parts of return expressions are removed, resulting in lower-dimensional input vectors.
\end{enumerate}
